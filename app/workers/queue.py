"""
Configuraci√≥n de Celery y tareas de background para ScrapingWeb

Este m√≥dulo define:
- Configuraci√≥n de la aplicaci√≥n Celery
- Tarea programada para scraping diario
- Tarea de procesamiento de NLP
"""

from celery import Celery
from celery.schedules import crontab
from sqlalchemy.orm import Session

from app.core.config import settings
from app.db.session import SessionLocal
from app.services.scraping_service import ScrapingService
from app.services.nlp_service import NLPService
from app.crud.crud_resena import crud_resena
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Crear instancia de Celery
celery_app = Celery(
    "scrapingweb",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

# Configuraci√≥n de Celery
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='America/Bogota',  # Ajustar a tu zona horaria
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30 minutos m√°ximo por tarea
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=100,
)

# Configuraci√≥n del scheduler (Beat)
celery_app.conf.beat_schedule = {
    'scraping-diario': {
        'task': 'app.workers.queue.ejecutar_scraping_diario',
        'schedule': crontab(hour=2, minute=0),  # Ejecutar a las 2:00 AM todos los d√≠as
        'args': (),
    },
}


@celery_app.task(name='app.workers.queue.ejecutar_scraping_diario')
def ejecutar_scraping_diario():
    """
    Tarea programada que ejecuta el scraping diario de todas las plataformas
    
    Esta tarea se ejecuta autom√°ticamente cada d√≠a a las 2:00 AM.
    Procesa las rese√±as de Google, Booking y Airbnb, y las almacena en la BD.
    """
    logger.info("üöÄ Iniciando scraping diario programado")
    
    db: Session = SessionLocal()
    try:
        scraping_service = ScrapingService(db)
        
        # Ejecutar scraping de todas las plataformas
        resultados = scraping_service.importar_desde_json()
        
        logger.info(f"‚úÖ Scraping completado: {resultados}")
        
        # Procesar NLP de las nuevas rese√±as
        procesar_nlp_nuevas_resenas.delay()
        
        return {
            "status": "success",
            "mensaje": "Scraping diario ejecutado correctamente",
            "resultados": resultados
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en scraping diario: {str(e)}")
        return {
            "status": "error",
            "mensaje": f"Error en scraping: {str(e)}"
        }
    finally:
        db.close()


@celery_app.task(name='app.workers.queue.procesar_nlp_nuevas_resenas')
def procesar_nlp_nuevas_resenas():
    """
    Tarea que procesa el an√°lisis NLP de rese√±as sin procesar
    
    Esta tarea es llamada despu√©s del scraping para analizar
    el sentimiento y clasificar las nuevas rese√±as.
    """
    logger.info("üß† Iniciando procesamiento NLP de nuevas rese√±as")
    
    db: Session = SessionLocal()
    try:
        nlp_service = NLPService()
        
        # Obtener rese√±as sin an√°lisis de sentimiento
        resenas_sin_procesar = crud_resena.get_sin_sentimiento(db)
        
        procesadas = 0
        errores = 0
        
        for resena in resenas_sin_procesar:
            try:
                # Analizar sentimiento
                resultado_sentimiento = nlp_service.analizar_sentimiento(resena.comentario)
                
                # Clasificar por criterios
                clasificaciones = nlp_service.clasificar_por_criterios(resena.comentario)
                
                # Actualizar rese√±a con an√°lisis
                crud_resena.actualizar_analisis_nlp(
                    db=db,
                    resena_id=resena.id,
                    sentimiento_data=resultado_sentimiento,
                    clasificaciones=clasificaciones
                )
                
                procesadas += 1
                
            except Exception as e:
                logger.error(f"Error procesando rese√±a {resena.id}: {str(e)}")
                errores += 1
        
        db.commit()
        
        logger.info(f"‚úÖ NLP completado: {procesadas} procesadas, {errores} errores")
        
        return {
            "status": "success",
            "procesadas": procesadas,
            "errores": errores
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en procesamiento NLP: {str(e)}")
        db.rollback()
        return {
            "status": "error",
            "mensaje": f"Error en NLP: {str(e)}"
        }
    finally:
        db.close()


@celery_app.task(name='app.workers.queue.ejecutar_scraping_manual')
def ejecutar_scraping_manual():
    """
    Tarea manual para ejecutar scraping bajo demanda
    
    Esta tarea puede ser invocada desde el endpoint de la API
    para forzar un scraping inmediato (por ejemplo, para testing).
    """
    logger.info("üîß Scraping manual solicitado")
    return ejecutar_scraping_diario()


@celery_app.task(name='app.workers.queue.calcular_indicadores')
def calcular_indicadores():
    """
    Tarea para recalcular todos los indicadores y m√©tricas
    
    Esta tarea puede ejecutarse despu√©s del an√°lisis NLP
    para actualizar todos los indicadores del sistema.
    """
    logger.info("üìä Recalculando indicadores")
    
    db: Session = SessionLocal()
    try:
        from app.services.indicadores_service import IndicadoresService
        
        indicadores_service = IndicadoresService(db)
        
        # Aqu√≠ se pueden agregar llamadas a m√©todos espec√≠ficos
        # de c√°lculo de indicadores seg√∫n la l√≥gica de negocio
        
        logger.info("‚úÖ Indicadores recalculados")
        
        return {
            "status": "success",
            "mensaje": "Indicadores actualizados"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error calculando indicadores: {str(e)}")
        return {
            "status": "error",
            "mensaje": f"Error: {str(e)}"
        }
    finally:
        db.close()
